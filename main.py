# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14zV6D6ZayGdT6fFjvkYNX8g7eH7QPKb6

### First we will convert the audio into text
"""

!pip install johnsnowlabs

from johnsnowlabs import nlp # Wrap around Spark NLP and NLU
from johnsnowlabs import medical # Wrap around Healthcare NLP
from johnsnowlabs import finance # Wrap around Finance NLP
from johnsnowlabs import legal # Wrap around Legal NLP
from johnsnowlabs import viz # Wrap around Spark NLP Display

val = nlp.load('en.speech2text.wav2vec2.v2_base_960h').predict("/content/obama.mp3")

print(val['text'][0])

"""### We will split the text into small chunks

"""

spc = 0
txt = val['text'][0]
txt_chunks = []
start = 0
for i in range(len(txt)):
  if spc == 4:
    spc = 0
    txt_chunks.append(txt[start:i])
    start = i
    continue
  if txt[i] == " ":
    spc += 1
print(txt_chunks)

"""### Now we will pass the sentence to be gramatically checked"""

!pip install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git
!pip install spacy
!python -m spacy download en

!wget https://github.com/bakwc/JamSpell-models/raw/master/en.tar.gz
!tar -xvf en.tar.gz
!!sudo apt-get install swig3.0
!sudo pip install jamspell

import jamspell
jsp = jamspell.TSpellCorrector()
assert jsp.LoadLangModel('en.bin')
txt_chunks_corrected = []
for i in txt_chunks:
  print(jsp.FixFragment(i))
  txt_chunks_corrected.append(jsp.FixFragment(i))

"""### Now we will translate the code into hindi and extract the pronunciation of it which is nothing but the hinglish version"""

!pip install googletrans==3.1.0a0

import googletrans
from googletrans import Translator
translator = Translator()
txt = txt_chunks_corrected
result_hindi = translator.translate(txt, dest='hi')
for i in result_hindi:
  print(i.pronunciation)

"""### Now we will clone the audio into teh specified voice and extract audio samples."""

!pip install bark
!pip install git+https://github.com/suno-ai/bark.git
!git clone https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer
!pip install -r ./bark-voice-cloning-HuBERT-quantizer/requirements.txt

import sys
sys.path.append('./bark-voice-cloning-HuBERT-quantizer')
import os
from scipy.io.wavfile import write as write_wav
import numpy as np
import torch
import torchaudio
from bark.api import generate_audio
from bark.generation import SAMPLE_RATE, preload_models, load_codec_model
from encodec.utils import convert_audio
from bark_hubert_quantizer.customtokenizer import CustomTokenizer
from bark_hubert_quantizer.hubert_manager import HuBERTManager
from bark_hubert_quantizer.pre_kmeans_hubert import CustomHubert

device = 'cuda' if torch.cuda.is_available() else 'cpu'
use_gpu = True if device == 'cuda' else False
model = load_codec_model(use_gpu=use_gpu)

preload_models(
    text_use_gpu=use_gpu,
    text_use_small=False,
    coarse_use_gpu=use_gpu,
    coarse_use_small=False,
    fine_use_gpu=use_gpu,
    fine_use_small=False,
    codec_use_gpu=use_gpu,
    force_reload=False
)

hubert_manager = HuBERTManager()
hubert_manager.make_sure_hubert_installed()
hubert_manager.make_sure_tokenizer_installed()

# Load the HuBERT model
hubert_model = CustomHubert(checkpoint_path='data/models/hubert/hubert.pt').to(device)

# Load the CustomTokenizer model
tokenizer = CustomTokenizer.load_from_checkpoint('data/models/hubert/tokenizer.pth', map_location=device).to(device)

for i in range(len(result_hindi)):
  text_prompt = result_hindi[i]
  audio_filepath = '/content/obama10.mp3'

  if not os.path.isfile(audio_filepath):
    raise ValueError(f"Audio file not exists ({audio_filepath})")

  wav, sr = torchaudio.load(audio_filepath)
  wav = convert_audio(wav, sr, model.sample_rate, model.channels)
  wav = wav.to(device)

  semantic_vectors = hubert_model.forward(wav, input_sample_hz=model.sample_rate)
  semantic_tokens = tokenizer.get_token(semantic_vectors)

  # Extract discrete codes from EnCodec
  with torch.no_grad():
      encoded_frames = model.encode(wav.unsqueeze(0))
  codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()

  # move codes to cpu
  codes = codes.cpu().numpy()
  # move semantic tokens to cpu
  semantic_tokens = semantic_tokens.cpu().numpy()

  voice_filename = 'output.npz'

  current_path = os.getcwd()
  voice_name = os.path.join(current_path, voice_filename)

  np.savez(voice_name, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)

  # simple generation
  audio_array = generate_audio(text_prompt, history_prompt=voice_name, text_temp=0.7, waveform_temp=0.7)

  # save audio
  filepath = "/content/out" + str(i) + ".wav" # change this to your desired output path
  write_wav(filepath, SAMPLE_RATE, audio_array)

"""### You can view the out(1 to n).wav files under content and download them"""

